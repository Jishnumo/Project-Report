<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Report</title>
    <link rel="stylesheet" href="style.css" />
  </head>
  <body>
    <!--navigation starts-->
    <nav>
      <h1>Report</h1>
      <ul>
        <li><a href="#one">Home</a></li>
        <li><a href="#two">Dataset</a></li>
        <li><a href="#three">Setup</a></li>
        <li><a href="#">Performance</a></li>
      </ul>
    </nav>
    <header id="one">
      <div class="write">
        <h1>
          Speaker Recognition using<br /><span
            >Python and Machine Learning</span
          >
        </h1>
        <p>
          “Using machine learning techniques to accurately identify speakers
          from audio recordings and develop a robust Speaker Recognition
          system.”The primary aim of this project is to create a Speaker
          Recognition system that can accurately differentiate between different
          speakers by analyzing their distinctive voice traits. To achieve this,
          we will employ sophisticated machine-learning techniques and a varied
          collection of audio recordings. Our focus is on achieving a high level
          of precision in speaker identification tasks in a range of real-world
          situations. We will conduct extensive training and testing to
          contribute to the advancement of Speaker Recognition technology for
          practical use in security, telecommunications, and personalized user
          experiences.
        </p>
        <button><a href="#teamm">Know more</a></button>
      </div>
      <div class="i"></div>
    </header>
    <section class="team" id="teamm">
      <h1>Team<span> details</span></h1>
      <div class="t">
        <div class="t1">
          <div class="i1 img"></div>
          <div class="w">
            <h3>Joe Gisto<br /><span>joegisto@gmail.com</span></h3>
          </div>
        </div>
        <div class="t2">
          <div class="i2 img"></div>
          <div class="w">
            <h3>
              Jishnu Mohan<br />
              <span>jm.csb2125@saintgits.org</span>
            </h3>
          </div>
        </div>
        <div class="t3">
          <div class="i3 img"></div>
          <div class="w">
            <h3>
              Navaneeth Krishna S<br /><span>nks.csb2125@saintgits.org</span>
            </h3>
          </div>
        </div>
      </div>
    </section>
    <section class="dataset" id="two">
      <h1>Dataset <span>Details</span></h1>
      <p class="io">
        The Speaker Recognition system's training dataset was procured from the
        reputable data science platform, Kaggle. The dataset comprises a
        comprehensive suite of audio recordings featuring voice samples from
        prominent political figures, including Benjamin Netanyahu, Jens
        Stoltenberg, Julia Gillard, Margaret Thatcher, Nelson Mandela, and
        others. The dataset also includes background noise recordings to emulate
        real-world scenarios, thereby enhancing the system's robustness. The
        dataset's size and diversity make it an invaluable resource for training
        and testing speaker recognition models. The link for the dataset
        download is given below.
      </p>
      <div class="b">
        <button>
          <a
            href="https://www.kaggle.com/datasets/kongaevans/speaker-recognition-dataset"
            >Link</a
          >
        </button>
      </div>
      <h2>Key Dataset <span>Characteristics</span></h2>
      <div class="nnn">
        <p class="iii">
          •Source: Kaggle <br />•Content: Political voice samples & background
          noise recordings. <br />•The audio format used is PCM. <br />•The
          sampling rate of the audio is 16,000 Hz. <br />
          •The dataset includes speakers of different genders, nationalities,
          accents, and speech patterns to ensure the system’s ability to
          generalize across diverse demographics. <br />
          • Each audio recording is labelled with the corresponding speaker’s
          identity or designated as background noise, which helps with
          supervised learning during model training and evaluation.
        </p>
        <h2>Preparing the <span>dataset</span></h2>
        <p class="i22">
          The Speaker Recognition system is first trained on a pre-processed
          dataset to ensure that it is compatible with the model architecture
          and training process. To convert audio recordings into numerical
          representations, feature extraction techniques such as MFCCs, chroma
          features, and mel spectrograms are used. These features capture
          important characteristics of the speakers' voices, making it possible
          for the system to learn discriminative patterns for speaker
          identification.
        </p>
        <h2>Dataset <span>Split</span></h2>
        <p class="i33">
          The dataset is usually split into two separate sets, namely, training
          and testing sets. This division enables the model to be trained and
          evaluated more efficiently. Normally, around 80% of the dataset is
          assigned to the training set while the remaining part is used for
          testing. Randomization techniques can be employed to ensure that the
          distribution of speakers in the training and testing sets is unbiased.
        </p>
        <h2>Incorporating <span>Background Noise</span></h2>
        <p class="i44">
          To make the system more resilient and useful in real-world situations,
          we have included background noise recordings in the dataset. These
          recordings simulate different acoustic environments and environmental
          conditions that the system might come across while in use. By training
          on a varied set of audio samples, which includes both speech and
          background noise, the system can differentiate between relevant speech
          signals and extraneous noise, resulting in better performance in
          practical scenarios.
        </p>
      </div>
      <div class="method" id="three">
        <h1>Method or <span>Experimental Setup</span></h1>
        <h2>Feature <span>Extraction</span></h2>
        <p class="i222">
          Through the utilization of the librosa library in Python, pertinent
          audio features can be extracted from raw audio recordings. These
          features include Mel-Frequency Cepstral Coefficients (MFCCs), chroma
          features, and mel spectrogram features. These characteristics are
          pivotal in capturing crucial aspects of the speaker's voice and can
          act as input parameters for the machine learning model.
        </p>
        <h2>Data <span>Preprocessing</span></h2>
        <p class="i222">
          The preprocessing of audio features is a crucial step in the
          standardization and preparation of the data for model input. To
          achieve this goal, the audio features undergo normalization, scaling,
          and the formatting process to generate suitable arrays that can be
          used for model training. Preprocessing guarantees that the data is in
          a consistent format and is ready for the model to process it without
          any inconsistencies or errors. By following these preprocessing steps,
          the audio features can be optimized and utilized effectively, thereby
          delivering improved results in the model's output.
        </p>
        <h2>Model <span>Architecture</span></h2>
        <p class="i222">
          The present study introduces a deep neural network architecture,
          designed through the employment of TensorFlow's Keras API. The model
          comprises several layers of densely connected neurons, which are
          facilitated with activation functions, including ReLU, to introduce
          non-linearity. The output layer employs the softmax function, which is
          instrumental in predicting the probabilities of each speaker class.
          The implementation of these components is critical, as it leads to the
          accurate prediction of speaker classes in a given dataset.
        </p>
        <h2>Training <span>Configuration</span></h2>
        <p class="i222">
          The presented machine learning model has been trained to utilize the
          Adam optimizer along with a sparse categorical cross-entropy loss
          function. To prevent overfitting, the dataset has been partitioned
          into training and validation sets. We have undertaken a comprehensive
          experimentation and validation exercise to optimize hyperparameters
          such as batch size, learning rate, and number of epochs. Our findings
          indicate that the resulting model is expected to deliver optimal
          performance, given the selected hyperparameters and training data
        </p>
        <h2>Evaluation</h2>
        <p class="i222">
          Upon completion of the training process, the model is subjected to
          evaluation using a separate test set composed of audio samples that
          were not used during the training phase. The performance of the model
          is assessed using a set of standard evaluation metrics, which include
          accuracy, precision, recall, and F1-score. These evaluation metrics
          are critical in evaluating the model's ability to accurately identify
          speakers. Precision measures the ratio of correctly identified
          positive instances to the total number of positive instances. Recall,
          on the other hand, measures the ratio of the number of actual positive
          instances that were accurately classified to the total number of
          positive instances. The F1-score represents the harmonic mean of
          precision and recall, which provides a comprehensive evaluation of the
          model's performance. These evaluation metrics are essential in
          determining the efficacy of the model in accurately identifying
          speakers. As such, they play a crucial role in assessing the model's
          overall performance.
        </p>
      </div>
    </section>
  </body>
</html>
